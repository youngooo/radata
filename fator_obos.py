import pandas as pd
import rqdatac
from sqlalchemy import create_engine, text
import pymysql
from datetime import datetime, timedelta
import time
import logging

# ============== åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ ==============
class DataBase_Position:
    __HOST = "106.15.107.93"
    __PORT = 3306
    __USERNAME = "chennianzeng"
    __PASSWORD = "chennianzeng666"
    __SCHEMA_OTC = "StockSignal"

    def __init__(self):
        self.conn = pymysql.connect(
            host=self.__HOST,
            port=self.__PORT,
            user=self.__USERNAME,
            password=self.__PASSWORD,
            charset="utf8",
            database=self.__SCHEMA_OTC
        )
        self.engine = create_engine(
            f"mysql+pymysql://{self.__USERNAME}:{self.__PASSWORD}@{self.__HOST}:{self.__PORT}/{self.__SCHEMA_OTC}?charset=utf8mb4"
        )

table_name = "factor_obos"
factor_type = "obos_indicator"
logger_file = "obos.log"

# ============== å‡½æ•°ï¼šåˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ ==============
def create_alpha101_table(db):
    create_sql = f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        order_book_id VARCHAR(20),
        date DATE,
        factor_name VARCHAR(50),
        factor_value DOUBLE,
        update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        PRIMARY KEY (order_book_id, date, factor_name)
    );
    """
    with db.engine.connect() as conn:
        conn.execute(text(create_sql))
        conn.commit()

# ============== é…ç½®æ—¥å¿— ==============
def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(logger_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

# ============== å‡½æ•°ï¼šè·å–æ—¥æœŸåˆ—è¡¨ ==============
def get_date_range(start_date, end_date):
    """ä½¿ç”¨rqdatacè·å–çœŸå®çš„äº¤æ˜“æ—¥åˆ—è¡¨"""
    try:
        # å°†æ—¥æœŸæ ¼å¼ä» 'YYYY-MM-DD' è½¬æ¢ä¸º 'YYYYMMDD'
        start_date_str = start_date.replace('-', '')
        end_date_str = end_date.replace('-', '')
        
        # ä½¿ç”¨rqdatacè·å–äº¤æ˜“æ—¥åˆ—è¡¨
        trade_dt_list = rqdatac.get_trading_dates(start_date=start_date_str, end_date=end_date_str)
        
        # å°†datetimeå¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ 'YYYY-MM-DD'
        date_list = [dt.strftime('%Y-%m-%d') for dt in trade_dt_list]
        
        return date_list
        
    except Exception as e:
        logging.error(f"è·å–äº¤æ˜“æ—¥åˆ—è¡¨å¤±è´¥: {e}")
        # å¦‚æœrqdatacè·å–å¤±è´¥ï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹æ³•
        logging.warning("å›é€€åˆ°æ‰‹åŠ¨è®¡ç®—äº¤æ˜“æ—¥")
        return get_date_range_fallback(start_date, end_date)

def get_date_range_fallback(start_date, end_date):
    """å›é€€æ–¹æ¡ˆï¼šæ‰‹åŠ¨ç”Ÿæˆæ—¥æœŸèŒƒå›´åˆ—è¡¨ï¼Œè·³è¿‡å‘¨æœ«"""
    start = datetime.strptime(start_date, '%Y-%m-%d')
    end = datetime.strptime(end_date, '%Y-%m-%d')
    
    date_list = []
    current = start
    while current <= end:
        # è·³è¿‡å‘¨æœ«ï¼ˆå‘¨å…­=5ï¼Œå‘¨æ—¥=6ï¼‰
        if current.weekday() < 5:
            date_list.append(current.strftime('%Y-%m-%d'))
        current += timedelta(days=1)
    
    return date_list

# ============== å‡½æ•°ï¼šæ•°æ®æ¸…æ´— ==============
def clean_factor_data(df_long, logger, batch_info=""):
    """æ¸…æ´—å› å­æ•°æ®ï¼Œå¤„ç†æ— ç©·å¤§å€¼å’Œç¼ºå¤±å€¼"""
    if df_long.empty:
        return df_long
    
    # ç»Ÿè®¡åŸå§‹æ•°æ®ä¸­çš„ç‰¹æ®Šå€¼
    inf_count = df_long['factor_value'].isin([float('inf'), float('-inf')]).sum()
    nan_count = df_long['factor_value'].isna().sum()
    
    if inf_count > 0 or nan_count > 0:
        logger.info(f"ğŸ”§ {batch_info} æ•°æ®æ¸…æ´—: infå€¼={inf_count}, nanå€¼={nan_count}")
    
    # å°†infå€¼æ›¿æ¢ä¸ºNoneï¼ˆMySQLä¸­å­˜å‚¨ä¸ºNULLï¼‰
    df_long['factor_value'] = df_long['factor_value'].replace([float('inf'), float('-inf')], None)
    
    # ç¡®ä¿å…³é”®å­—æ®µä¸ä¸ºç©º
    df_cleaned = df_long.dropna(subset=['order_book_id', 'date', 'factor_name'])
    
    # è®°å½•æ¸…æ´—åçš„æ•°æ®é‡
    if len(df_cleaned) != len(df_long):
        logger.info(f"ğŸ§¹ {batch_info} æ¸…æ´—åæ•°æ®é‡: {len(df_cleaned)}/{len(df_long)}")
    
    return df_cleaned

# ============== å‡½æ•°ï¼šæ£€æŸ¥æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æ—¥æœŸ ==============
def get_existing_dates(db, start_date, end_date):
    """è·å–æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æ—¥æœŸ"""
    try:
        existing_dates = pd.read_sql(
            f"SELECT DISTINCT date FROM {table_name} WHERE date BETWEEN '{start_date}' AND '{end_date}'",
            con=db.engine
        )['date'].tolist()
        return [str(date) for date in existing_dates]
    except Exception as e:
        logging.warning(f"è·å–å·²å­˜åœ¨æ—¥æœŸæ—¶å‡ºé”™: {e}")
        return []

# ============== å‡½æ•°ï¼šå•æ—¥æ•°æ®è·å– ==============
def fetch_single_day_factors(db, target_date, batch_size=100, max_retries=3):
    """è·å–å•æ—¥çš„å› å­æ•°æ®"""
    logger = logging.getLogger(__name__)
    
    # æ£€æŸ¥è¯¥æ—¥æœŸæ˜¯å¦å·²å­˜åœ¨
    existing_dates = get_existing_dates(db, target_date, target_date)
    if target_date in existing_dates:
        logger.info(f"ğŸ“… {target_date} æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
        return True
    
    try:
        # è·å–å› å­åå’Œè‚¡ç¥¨åˆ—è¡¨
        factor_list = rqdatac.get_all_factor_names(type=factor_type)
        all_stocks = rqdatac.all_instruments(type='CS', market='cn', date=target_date)['order_book_id'].tolist()
        
        logger.info(f"ğŸ“… å¼€å§‹è·å– {target_date} çš„æ•°æ®ï¼Œå…± {len(all_stocks)} åªè‚¡ç¥¨")
        
        total_inserted = 0
        
        for i in range(0, len(all_stocks), batch_size):
            stock_batch = all_stocks[i:i+batch_size]
            retry_count = 0
            
            while retry_count < max_retries:
                try:
                    df = rqdatac.get_factor(order_book_ids=stock_batch,
                                          factor=factor_list,
                                          start_date=target_date,
                                          end_date=target_date,
                                          expect_df=True)
                    
                    if df is None or df.empty:
                        logger.warning(f"âš ï¸ {target_date} batch {i//batch_size+1} è¿”å›ç©ºæ•°æ®")
                        break
                    
                    # è½¬æ¢ä¸ºé•¿è¡¨ï¼ˆorder_book_id, date, factor_name, factor_valueï¼‰
                    df_long = df.reset_index().melt(id_vars=['order_book_id', 'date'],
                                                   var_name='factor_name',
                                                   value_name='factor_value')
                    
                    # æ•°æ®æ¸…æ´—ï¼šå¤„ç†æ— ç©·å¤§å€¼å’Œç¼ºå¤±å€¼
                    if not df_long.empty:
                        batch_info = f"{target_date} batch {i//batch_size+1}/{len(all_stocks)//batch_size+1}"
                        df_cleaned = clean_factor_data(df_long, logger, batch_info)
                        
                        # æ’å…¥æ•°æ®åº“
                        if not df_cleaned.empty:
                            df_cleaned.to_sql(table_name, con=db.engine, if_exists='append', index=False)
                            total_inserted += len(df_cleaned)
                            logger.info(f"âœ… {batch_info} æ’å…¥ {len(df_cleaned)} è¡Œæ•°æ®")
                    
                    # æ·»åŠ çŸ­æš‚å»¶è¿Ÿé¿å…APIé™åˆ¶
                    time.sleep(0.1)
                    break
                    
                except Exception as e:
                    retry_count += 1
                    error_msg = str(e)
                    
                    # ç‰¹æ®Šå¤„ç†MySQLç›¸å…³é”™è¯¯
                    if "inf cannot be used with MySQL" in error_msg:
                        logger.error(f"âŒ {target_date} batch {i//batch_size+1} æ•°æ®åŒ…å«æ— ç©·å¤§å€¼ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        break  # è·³è¿‡æ­¤æ‰¹æ¬¡ï¼Œä¸é‡è¯•
                    elif "MySQL server has gone away" in error_msg:
                        logger.warning(f"âš ï¸ {target_date} batch {i//batch_size+1} MySQLè¿æ¥æ–­å¼€ï¼Œç­‰å¾…é‡è¿")
                        time.sleep(5)  # ç­‰å¾…æ›´é•¿æ—¶é—´é‡è¿
                    else:
                        logger.warning(f"âš ï¸ {target_date} batch {i//batch_size+1} ç¬¬ {retry_count} æ¬¡é‡è¯•ï¼Œé”™è¯¯: {e}")
                    
                    if retry_count >= max_retries:
                        logger.error(f"âŒ {target_date} batch {i//batch_size+1} é‡è¯• {max_retries} æ¬¡åå¤±è´¥")
                        break
                    time.sleep(2)  # é‡è¯•å‰ç­‰å¾…
        
        logger.info(f"ğŸ‰ {target_date} æ•°æ®è·å–å®Œæˆï¼Œå…±æ’å…¥ {total_inserted} è¡Œæ•°æ®")
        return True
        
    except Exception as e:
        logger.error(f"âŒ {target_date} æ•°æ®è·å–å¤±è´¥: {e}")
        return False

# ============== å‡½æ•°ï¼šæ‹‰å–å› å­æ•°æ®ï¼ˆå¸¦æ—¥æœŸè½®è¯¢ï¼‰ ==============
def fetch_and_insert_factors(db, start_date, end_date, batch_size=100):
    """ä¸»è¦çš„å› å­æ•°æ®è·å–å‡½æ•°ï¼Œæ”¯æŒæ—¥æœŸè½®è¯¢"""
    logger = setup_logging()
    logger.info(f"ğŸš€ å¼€å§‹è·å–å› å­æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
    
    create_alpha101_table(db)
    
    # è·å–æ‰€æœ‰äº¤æ˜“æ—¥
    trading_dates = get_date_range(start_date, end_date)
    logger.info(f"ğŸ“Š å…±éœ€å¤„ç† {len(trading_dates)} ä¸ªäº¤æ˜“æ—¥")
    
    success_count = 0
    failed_dates = []
    
    for i, date in enumerate(trading_dates, 1):
        logger.info(f"ğŸ“ˆ è¿›åº¦: {i}/{len(trading_dates)} - å¤„ç†æ—¥æœŸ: {date}")
        
        if fetch_single_day_factors(db, date, batch_size):
            success_count += 1
        else:
            failed_dates.append(date)
        
        # æ¯æ—¥å¤„ç†å®Œæˆåç¨ä½œä¼‘æ¯
        time.sleep(1)
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    logger.info(f"ğŸ æ•°æ®è·å–å®Œæˆï¼æˆåŠŸ: {success_count}/{len(trading_dates)} å¤©")
    if failed_dates:
        logger.warning(f"âš ï¸ å¤±è´¥çš„æ—¥æœŸ: {failed_dates}")
    
    return success_count, failed_dates

# ============== å‡½æ•°ï¼šé‡æ–°è·å–å¤±è´¥çš„æ—¥æœŸ ==============
def retry_failed_dates(db, failed_dates, batch_size=100):
    """é‡æ–°è·å–å¤±è´¥çš„æ—¥æœŸæ•°æ®"""
    if not failed_dates:
        return
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ”„ å¼€å§‹é‡æ–°è·å–å¤±è´¥çš„æ—¥æœŸ: {failed_dates}")
    
    for date in failed_dates:
        logger.info(f"ğŸ”„ é‡æ–°è·å– {date}")
        if fetch_single_day_factors(db, date, batch_size):
            logger.info(f"âœ… {date} é‡æ–°è·å–æˆåŠŸ")
        else:
            logger.error(f"âŒ {date} é‡æ–°è·å–ä»ç„¶å¤±è´¥")

# ============== ä¸»ç¨‹åº ==============
if __name__ == "__main__":
    # åˆå§‹åŒ– rqdatac
    rqdatac.init()  # æ›¿æ¢æˆä½ çš„ rqdatac è´¦å·
    
    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
    db = DataBase_Position()
    
    # è®¾ç½®æ—¥æœŸèŒƒå›´
    start_date = "2020-01-01"
    end_date = "2025-09-19"
    
    # è®¾ç½®æ‰¹å¤„ç†å¤§å°
    batch_size = 100
    
    try:
        # æ‰§è¡Œæ•°æ®è·å–
        success_count, failed_dates = fetch_and_insert_factors(db, start_date, end_date, batch_size)
        
        # å¦‚æœæœ‰å¤±è´¥çš„æ—¥æœŸï¼Œè¯¢é—®æ˜¯å¦é‡è¯•
        if failed_dates:
            print(f"\nâš ï¸ å‘ç° {len(failed_dates)} ä¸ªå¤±è´¥çš„æ—¥æœŸï¼Œæ˜¯å¦é‡è¯•ï¼Ÿ")
            retry_choice = input("è¾“å…¥ 'y' é‡è¯•ï¼Œå…¶ä»–é”®è·³è¿‡: ").lower().strip()
            if retry_choice == 'y':
                retry_failed_dates(db, failed_dates, batch_size)
        
        print(f"\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
        
    except Exception as e:
        logging.error(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        if hasattr(db, 'conn'):
            db.conn.close()
        print("ğŸ“ æ•°æ®åº“è¿æ¥å·²å…³é—­")
